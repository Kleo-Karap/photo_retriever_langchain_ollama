{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download sample photos:"
      ],
      "metadata": {
        "id": "ZLKbS67l9OC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown, os\n",
        "destination_folder = \"./photos\"\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "drive_folder_url = \"https://drive.google.com/drive/folders/156JvLn-CLIdUGmRxumr2ZtbPGFND_k0d\"\n",
        "gdown.download_folder(url=drive_folder_url, output=destination_folder, quiet=False, use_cookies=False)"
      ],
      "metadata": {
        "id": "6yteTjOT9TWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup - Files, Ollama and Packages"
      ],
      "metadata": {
        "id": "kBBUT_EPgmk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CHgW9soinCac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files Upload"
      ],
      "metadata": {
        "id": "FjAbR3vdiQV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's add our photos. In the notebook, create a folder named '**photos**'. Upload your files in the folder.\n",
        "\n",
        "If you don't have some photos handy, [get some from here.](https://drive.google.com/drive/folders/156JvLn-CLIdUGmRxumr2ZtbPGFND_k0d?usp=sharing)\n"
      ],
      "metadata": {
        "id": "o8PFAx0UsV6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ollama Installation"
      ],
      "metadata": {
        "id": "ahyZf_JZiUQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's continue with installing Ollama. To do so, we must install the colab-xterm package. This will provide us with a CLI, from where we can perform the required actions."
      ],
      "metadata": {
        "id": "ep-BdUlhvcCm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEdI1cOqryI3"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we need to do now is:\n",
        "\n",
        "\n",
        "\n",
        "1.   Open a terminal\n",
        "2.   Install and run Ollama\n",
        "\n",
        "1.   Pull the models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dh5VgLeIvwxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must run execute some commands in the terminal.\n",
        "\n",
        "To install Ollama:\n",
        "\n",
        "`curl -fsSL https://ollama.ai/install.sh | sh`\n",
        "\n",
        "To run Ollama:\n",
        "\n",
        "`ollama serve &`\n",
        "\n",
        "To pull the vision model:\n",
        "\n",
        "`ollama pull llava:7b`\n",
        "\n",
        "We also need to pull an **embedding model**.\n",
        "\n",
        "`ollama pull mxbai-embed-large`\n",
        "\n",
        "Model links for reference:\n",
        "\n",
        "[llava](https://ollama.com/library/llava)\n",
        "\n",
        "[mxbai-embed-large](https://ollama.com/library/mxbai-embed-large)\n"
      ],
      "metadata": {
        "id": "gQn6hjk3wnqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "JaRooKCFvYWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If needed, run this:"
      ],
      "metadata": {
        "id": "8pykJ5azHsOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXi4ZbtgHhuK"
      },
      "outputs": [],
      "source": [
        "%reload_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm our installation. In the terminal window, type:\n",
        "\n",
        "`ollama run llava:7b`\n",
        "\n",
        "Ollama will load and execute our model. Say hello and the model will respond. To exit the chat type:\n",
        "\n",
        "`/bye`\n"
      ],
      "metadata": {
        "id": "b9bUILBc0eDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "3w_ckQLGjHx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's install the packages we require.\n",
        "\n",
        "(Please note that we will also install the **OpenAI** package - this is a backup plan, just in case GPU resources will not be available in Colab)."
      ],
      "metadata": {
        "id": "4uWGkzqM4-36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-chroma langchain-ollama pillow langchain-openai"
      ],
      "metadata": {
        "id": "wDhHY_0V4-M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test our installation"
      ],
      "metadata": {
        "id": "oHlt1bYZjoVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to query our model with Langchain.\n",
        "\n",
        "To do this, we need to configure a **chain**.\n",
        "\n",
        "A typical chain is made from 3 parts:\n",
        "\n",
        "`prompt -> model -> output formatting`\n",
        "\n",
        "Let's break those down one by one."
      ],
      "metadata": {
        "id": "RvQ_K7Vt7B2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "7EEANyVIlTnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) object. In our case, this is nothing more than a [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) and a [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)."
      ],
      "metadata": {
        "id": "Qu8UcA3RlZwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    ('system', 'You are a helpful AI assistant.'),\n",
        "    ('human', '{query}')])"
      ],
      "metadata": {
        "id": "YwCThew891SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "014wu3-cllaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's setup our model.\n",
        "This is very simple and can be switched to another model extremely easily."
      ],
      "metadata": {
        "id": "531g705tC7mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "llm = ChatOllama(model='llava:7b')\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# llm = ChatOpenAI(model='gpt-4o', api_key='<your API key>')"
      ],
      "metadata": {
        "id": "oWbJ5kXxC-nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to show you that we could invoke the model directly:"
      ],
      "metadata": {
        "id": "W_MHq5NN_3g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_response = llm.invoke('A penguin and an astronaut walk into a bar. Fill in the rest, make it short.')\n",
        "print(test_response)"
      ],
      "metadata": {
        "id": "t-cMlC9u_cyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However we want to do this properly, so we will proceed with the more nuanced ChatPromptTemplate."
      ],
      "metadata": {
        "id": "JwyaxZhXACOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "po09BWHXl29z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to add the [output parser](https://python.langchain.com/docs/concepts/output_parsers/) we want. Output parsing dictates the output format we want from the model. There are several options here. In our example, we just get a string back. Complex outputs are supported, for example Json, with the caveat that you most probably require a bigger model."
      ],
      "metadata": {
        "id": "23k77FKzDPEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "HHpnXzhfDgT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Invoke the chain"
      ],
      "metadata": {
        "id": "KR_Ddr9Il92P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have all the pieces to create our example chain."
      ],
      "metadata": {
        "id": "4J4yTAZYDZKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = template | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "bJITHltSDtQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to ask something:"
      ],
      "metadata": {
        "id": "LWS0El8O7jUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = 'A penguin and an astronaut walk into a bar. Fill in the rest, keep it short and make it a bit weird.'\n",
        "response = chain.invoke({'query':user_query})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "UTTQr-OrD6lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done!"
      ],
      "metadata": {
        "id": "WJhnod9FJCiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plan of attack\n",
        "\n",
        "\n",
        "\n",
        "1.   Setup our embedding function\n",
        "2.   Setup our vector database\n",
        "1.   Setup our vision chain\n",
        "2.   For each photo: analyze the photo and store the result in the database\n",
        "\n",
        "Some of those steps are a bit more involved, so let's take those one by one.\n"
      ],
      "metadata": {
        "id": "3jsLeMy1Nbkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps 1 & 2 - Embeddding Function and Vector Database\n",
        "\n",
        "We will use the `mxbai-embed-large` embedding model that we pulled into Ollama previously. We will also create a Chroma database  with that embedding function.\n"
      ],
      "metadata": {
        "id": "3VPPr8pKOe49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "embedding_function = OllamaEmbeddings(model='mxbai-embed-large')\n",
        "db = Chroma(\n",
        "    collection_name='photo_collection',\n",
        "    embedding_function=embedding_function,\n",
        "    persist_directory='./db_photos')"
      ],
      "metadata": {
        "id": "XSilT4b5I-si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Chain Setup\n",
        "\n",
        "Now we need to configure our chains. This is going to be a bit more complicated than in the previous simple text example. There are several prerequisites for this, because now we will work with images instead of text.\n",
        "\n",
        "The way this works is that we convert the photo bytes to a base64 string, and we craft a special HumanMessage, containing both the image data and the user's request for the model.\n",
        "\n",
        "Let's get the simple things out of the way first.\n",
        "\n"
      ],
      "metadata": {
        "id": "wlyqj15AQGLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System and Human prompts"
      ],
      "metadata": {
        "id": "VCc1_nqssmZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vision_system_message()->str:\n",
        "    system_message_text = '''\n",
        "You're an expert image and photo analyzer.\n",
        "You are very perceptive in analyzing images and photos.\n",
        "You possess excelent vision.\n",
        "Do not read any text unless it is the most prominent in the image.\n",
        "Your description should be neutral in tone.\n",
        "'''\n",
        "    return system_message_text\n"
      ],
      "metadata": {
        "id": "-PmatrcxR4Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function just returns the system prompt text, and is just a convenient wrapper. You could just have that in a constant if you want.\n",
        "\n",
        "For our use case, let's also create a similar function that will return the HumanMessage's text.\n",
        "\n",
        "**Important Note**: This would not be the case if we were building, for example a chatbot, where the human message would be typed by an actual human. In that case, we would retrieve the user input and just add it dynamically. Here however, it is always going to be the same, so let's make our lives a bit easier and wrap this into a function too."
      ],
      "metadata": {
        "id": "LlcT5mcmUVnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vision_human_message()->str:\n",
        "    human_message_text = 'Describe the image in as much detail as possible. Do not try to read any text.'\n",
        "    return human_message_text"
      ],
      "metadata": {
        "id": "ZtU8KGPFVMJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eL3eqseirs8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function - Convert a PIL image to a base64 string"
      ],
      "metadata": {
        "id": "GdclOnYwsx-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a utility function that will convert an image to a base64 string. We will use pillow for that."
      ],
      "metadata": {
        "id": "aWh9h9c_R85e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "def convert_to_base64(pil_image)->str:\n",
        "    buffered = BytesIO()\n",
        "    pil_image.save(buffered, format=\"JPEG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return img_str"
      ],
      "metadata": {
        "id": "kKyJYXVXSl04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6a6AHSBMzUeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Function"
      ],
      "metadata": {
        "id": "sTH1WIm4s9o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now comes the tricky part: we are going to construct the prompt that we will use. As we have touched upon this a bit already, we will create a SystemMessage and a HumanMessage.\n",
        "\n",
        "The SystemMessage contains the base directions for the model. We have those already in the `get_vision_system_message()` function.\n",
        "\n",
        "The HumanMessage is a bit more tricky: we need to include both the base64 data and the user's prompt.\n",
        "\n",
        "To do this, we will create two **Parts**. A Part is just a python dictionary, containing the **type** and the **value** of the part. For example, a text part would be something like:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    'type': 'text',\n",
        "    'text': 'The actual value of the part'\n",
        "}\n",
        "```\n",
        "\n",
        "While an image part would be something like:\n",
        "\n",
        "```\n",
        "# Ollama\n",
        "{\n",
        "    'type': 'image_url',\n",
        "    'image_url': f'data:image/jpeg;base64,<the base64 data>'\n",
        "}\n",
        "```\n",
        "\n",
        "For OpenAI it is slightly different:\n",
        "\n",
        "```\n",
        "# OpenAI\n",
        "{\n",
        "    'type': 'image_url',\n",
        "    'image_url': {\n",
        "      'url':f'data:image/jpeg;base64,<the base64 data>'\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's build our prompt function:"
      ],
      "metadata": {
        "id": "nNifanZKTc-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_func(data):\n",
        "    imageb64 = data[\"imageb64\"]\n",
        "    system_message = SystemMessage(content=get_vision_system_message())\n",
        "    text_part = {\"type\": \"text\", \"text\": get_vision_human_message()}\n",
        "    # Ollama\n",
        "    image_part = {\n",
        "        'type': 'image_url',\n",
        "        'image_url': f'data:image/jpeg;base64,{imageb64}',\n",
        "    }\n",
        "    # OpenAI\n",
        "    # image_part = {\n",
        "    #    'type': 'image_url',\n",
        "    #     'image_url': {'url':f'data:image/jpeg;base64,{imageb64}'},\n",
        "    # }\n",
        "    content_parts = []\n",
        "    content_parts.append(image_part)\n",
        "    content_parts.append(text_part)\n",
        "    human_message = HumanMessage(content=content_parts)\n",
        "    return [system_message, human_message]\n"
      ],
      "metadata": {
        "id": "4szZjmkeYpO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we are doing here is:\n",
        "\n",
        "1.   We extract the base64 data  from the dictionary\n",
        "2.   We create our SystemMessage object\n",
        "1.   We create a text part containing the human message prompt text\n",
        "2.   We create an image part containing the base64 data (our actual image)\n",
        "1.   We add both parts to a list\n",
        "2.   We construct the HumanMessage object from this list\n",
        "1.   We return the combined System and Human messages that make up our prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V8ORt4A2rlvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a quick test - just replace the photo's file name/path."
      ],
      "metadata": {
        "id": "sjeGM683qzY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# TEST\n",
        "\n",
        "vision_chain = prompt_func | llm | StrOutputParser()\n",
        "\n",
        "# just replace the file name with one of your files\n",
        "file_name = './photos/100_3148.JPG'\n",
        "\n",
        "with Image.open(file_name) as img:\n",
        "  imageb64 = convert_to_base64(img)\n",
        "  response = vision_chain.invoke({'imageb64':imageb64})\n",
        "  print(response)"
      ],
      "metadata": {
        "id": "Il_9JoWIZUEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Main Loop"
      ],
      "metadata": {
        "id": "ppuN6sj9vVV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think a bit about how our main loop would look like:\n",
        "\n",
        "Retrieve the available photo paths and filenames\n",
        "\n",
        "\n",
        "```\n",
        "For each file:\n",
        "  convert to base64\n",
        "  get the model's analysis\n",
        "  create a Langchain document\n",
        "  store the Document in the database\n",
        "```\n",
        "\n",
        "Well this looks good, but it is a bit simplistic and not very useful. We should do a couple more things at least.\n",
        "\n",
        "We should add some metadata to the document. We should at least store the full path and name of the file. Optionally we can add more information that would be useful to us - for example we could add some photographic metadata, like the photo's date or even the location if it is available.\n",
        "\n",
        "By adding the full path and name of the file, we can do efficient processing and avoid constly LLM calls to reprocess the same photo.\n",
        "\n",
        "Our revised loop should look like:\n",
        "\n",
        "\n",
        "```\n",
        "For each file:\n",
        "  IF there is NO database entry:\n",
        "    convert to base64\n",
        "    get the model's analysis\n",
        "    create a Langchain document\n",
        "    store the Document in the database\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sqp7F2Cwvdy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Retrieval"
      ],
      "metadata": {
        "id": "ZLZH-7T6ybVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use glob to get a list of our files."
      ],
      "metadata": {
        "id": "83XprmwD4Y5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "photo_files = glob.glob(\"./photos/*\")\n",
        "for file in photo_files:\n",
        "  print(file)"
      ],
      "metadata": {
        "id": "1c5_F1PTv34k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final pieces to the puzzle"
      ],
      "metadata": {
        "id": "tE27sW6GI1ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reached a point where we only miss two things:\n",
        "\n",
        "\n",
        "\n",
        "1.   Store our analysis and metadata to the vector database\n",
        "2.   Check if a photo has already been processed.\n",
        "\n",
        "Once we have those in place, the only thing left to do is to piece everything together.\n",
        "\n"
      ],
      "metadata": {
        "id": "fDGTYGKBI83F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing into the vector database"
      ],
      "metadata": {
        "id": "5LDq0BVIJoYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain provides a very easy way to handle input and output in a vector database by providing the Document class. The way this works is that we instantiate a Document class, passing in the text we want to embed (in our case the model's description of the photo) and any additional metadata we want to include.\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "document = Document(\n",
        "    page_content=\"Hello, world!\",\n",
        "    metadata={\"source\": \"https://example.com\"}\n",
        ")\n",
        "```\n",
        "\n",
        "Our document is created, and now we should store it in our database. We have already initiated our vector database, `db`, and attached an embedding model (`mxbai-embed-large`) to it.\n",
        "\n",
        "```\n",
        "db.add_documents([document])\n",
        "```\n",
        "\n",
        "Note that add_documents takes a list.\n",
        "\n",
        "What this does is that it will create the vector for the text we provide at the **page_content** parameter, and attach the additional metadata to this database record.\n",
        "\n",
        "\n",
        "For more information check [Langchain Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)"
      ],
      "metadata": {
        "id": "nK3-KDGVJt3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this, we need to import the Document class:"
      ],
      "metadata": {
        "id": "OvtZUEn9MUXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "-v37ZxX6Mfv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Metadata"
      ],
      "metadata": {
        "id": "6iSo3rpDNOML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's talk about metadata. For every vector we store, we can accompany it with various metadata. In our case, we should definitely include the file name, because we are going to use it to check whether we have already processed a photo or not.\n",
        "\n",
        "While handy for our program, this is not very useful for our problem. Why not include additional information that may be useful? For example, we could include the data and time the photo was taken, plus any number of interesting facts about the photo, for example the Make and the Model of the camera used.\n",
        "\n",
        "This is all handy, because Langchain provides a very easy way to filter through the metadata before searching through the vector space.\n",
        "\n",
        "Let's see how we will do this - fortunately Pillow is quite good for this:"
      ],
      "metadata": {
        "id": "kWYOuyKGNRfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile\n",
        "from PIL.ExifTags import TAGS\n",
        "\n",
        "def get_exif_data(img:ImageFile)->dict:\n",
        "    exif_data = img._getexif()\n",
        "\n",
        "    if exif_data:\n",
        "        exif_dict = {}\n",
        "        for tag, value in exif_data.items():\n",
        "            tag_name = TAGS.get(tag, tag)\n",
        "            exif_dict[tag_name] = value\n",
        "        return exif_dict\n",
        "    else:\n",
        "        return {}\n",
        "\n",
        "def get_metadata(file_name:str, img:ImageFile)->dict:\n",
        "  exif = get_exif_data(img)\n",
        "  make = exif.get('Make', '')\n",
        "  model = exif.get('Model', '')\n",
        "  dt = exif.get('DateTime', '') or exif.get('DateTimeOriginal', '')\n",
        "  return {\n",
        "      'file_name':file_name,\n",
        "      'make':make,\n",
        "      'model':model,\n",
        "      'dt':dt\n",
        "  }"
      ],
      "metadata": {
        "id": "Kqb2BbabPGsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this:"
      ],
      "metadata": {
        "id": "HEkS9LJCRwQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# TEST\n",
        "\n",
        "file_name = './photos/20220814172719_IMG_9572.JPG'\n",
        "with Image.open(file_name) as img:\n",
        "  test = get_metadata(file_name, img)\n",
        "  print(test)"
      ],
      "metadata": {
        "id": "_vWLBdTaRWcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get files that have already been processed"
      ],
      "metadata": {
        "id": "kaqWcmxu4mCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way to get our processed file names is to simply retrieve the corresponding metadata field from the database, in our case `file_name`. To do this, we can directly query the database.\n",
        "\n",
        "Chroma is just a sqlite database - very handy, right?"
      ],
      "metadata": {
        "id": "sXA_d847A6-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import closing\n",
        "import sqlite3\n",
        "\n",
        "def get_processed_files():\n",
        "    with closing(sqlite3.connect(f\"./db_photos/chroma.sqlite3\")) as connection:\n",
        "        sql = \"select string_value from embedding_metadata where key='file_name'\"\n",
        "        rows = connection.execute(sql).fetchall()\n",
        "        processed_files = [file_name for file_name, in rows]\n",
        "    return processed_files"
      ],
      "metadata": {
        "id": "G6GJJr9QS3cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Loop"
      ],
      "metadata": {
        "id": "mksGBPvIT6TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the pieces are there. Time to assemble them:"
      ],
      "metadata": {
        "id": "q5lNrxS2UAzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_files = get_processed_files()\n",
        "for file_name in photo_files:\n",
        "  if file_name not in processed_files:\n",
        "    with Image.open(file_name) as img:\n",
        "      print(f'Processing: {file_name}')\n",
        "      imageb64 = convert_to_base64(img)\n",
        "      llm_analysis = vision_chain.invoke({'imageb64':imageb64})\n",
        "      metadata = get_metadata(file_name, img)\n",
        "      document = Document(page_content=llm_analysis, metadata=metadata)\n",
        "      db.add_documents([document])\n",
        "      print('OK')\n",
        "  else:\n",
        "    print(f'File {file_name} has already been processed')\n",
        "print('DONE!')"
      ],
      "metadata": {
        "id": "3h4bDRIWUJFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "bfWa-BL8JcXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to retrieve our documents is by using similarity search:"
      ],
      "metadata": {
        "id": "A1pHNrcMJi5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = db.similarity_search('jack', k=2)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "VJOrFLS0VmDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for doc in results:\n",
        "  print(doc.metadata['file_name'])\n",
        "  print(doc)"
      ],
      "metadata": {
        "id": "s1LvEfNaYOCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create a retriever:"
      ],
      "metadata": {
        "id": "nBK-d2LxJoil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "results = retriever.invoke('jack', k=2)"
      ],
      "metadata": {
        "id": "8o-sp2hFJtfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2TPhVEpd6-O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can initiate the retriever with the search type and threshold as well:"
      ],
      "metadata": {
        "id": "oFm-VWh_KLO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.2})\n",
        "results = retriever.invoke('jack', k=2)"
      ],
      "metadata": {
        "id": "MG2fRbz4KWQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can even filter our metadata:"
      ],
      "metadata": {
        "id": "oZDbadtvbd3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs={'filter': {'make':'Canon'}})\n",
        "results = retriever.invoke('', k=2)"
      ],
      "metadata": {
        "id": "TIaR1Uydbhsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Where to go from here"
      ],
      "metadata": {
        "id": "SXtuyTgUxAxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, you can check out the full example on my [Medium](https://medium.com/@dimosdennis/personal-photo-library-with-langchain-ollama-llava-fully-local-e82edfe07f54).\n",
        "\n",
        "You can take those basic principles and apply them to your use cases. RAG is the prime example - you can process some data (media or not), and do semantic retrieval and further generation. You could do some entity extraction and store them in a graph database.The world is your oyster.\n",
        "\n",
        "AI is expensive - while we examined local execution, processing time quickly adds up, and time is a valuable commodity.\n",
        "\n",
        "Enjoy!\n"
      ],
      "metadata": {
        "id": "Mp1iEMcQxIua"
      }
    }
  ]
}